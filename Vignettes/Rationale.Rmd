---
title: "Rationale"
output: html_document
date: "2025-09-14"
---

## Rationale 

-why method exists 
-walks through the by-hand processes I did for the internship that I turned into functions. maybe include the by hand code for the permanova stuff? explaining how we pulled hte sums of squares for each permutation and looped through to recalculate? or maybe not idk yet 
-set up the problem that the package solves ie makes it easier to adjust F tests 
-also include why adjusted F tests and variance partitioning are needed 
-link anderson papers here

F statistics compare the variation explained by a specified term against the estimate of background variation: $F = MS_{Factor}/MS_{Denominator}$. The goal is for the denominator to contain exactly the same variance components as the numerator, with the exeption of the term that is being tested. 

In simple models, the F test usually works out to the standard $MS_{Factor}/MS_{Residual}$. For fixed effects, the numerator contains fixed effect and residual variance, so to isolate the contributions of the fixed effect, dividing by the residual error is the most accurate test. For random, nested, and interaction factors, the variance partitioning becomes a little more complex and the denominator needs to be adjusted to cancel out the effect of interest in the numerator. If $MS_{Factor}/MS_{Residual}$ is used to assess significance for a chosen term when it is not ____, you could be left with additional variance not cancelled out in the numerator. This could lead to an inflated F statistic.

An easy example could be: 

$y_{ij} = \mu + A_i + B_{(i)j} + \varepsilon_{ij}$

Where: 
$y_{ij}$ = Species Composition
$\mu$ = Grand Mean of Species Composition 
$A_i$ = Factor A (Park) with $i$ Levels 
$B_{(i)j}$ = Factor B (Plot) 
$\varepsilon_{ij}$ = Residual Variation 

Assume the most accurate F test for testing the effects of Factor $A_i$ in the above example model is $F = MS_{FactorA}/MS_{FactorB}$ because both the numerator and denominator share variance from Factor B (which is a nested effect). But instead of selecting $MS_{FactorB}$ for the denominator, which contains both variation from Factor B and residiual variation, you select $MS_{Residial}$, which only contains residual variation and none from factor B. Now, the equation looks like this: $F_{Inaccurate} = MS_{FactorA}/MS_{Residual}$, and when you attempt to isolate the effects of Factor A, you are left with additional variation from factor B still present. This will lead to an inflated F value, which in turn could cause smaller P values to be inaccurately observed in your analysis. The null distribution would be shifted and it wolud increase the likelihood of false significance being concluded for factors when the variation could be explained by other sources (ie a nested or random effect that wasn't cancelled out in the calculations). 

Adjusting the F equations helps reduce Type 1 error problems and leads to higher confidence in the effects of the factor of interest. It helps you avoid concluding that the treatment of interest is driving the differences in species composition (for example) when really it was plot level differences or site variation (or possibly beta dispersion?)


The following workflow is an example of a full variance partitioning workflow for a nested model containing four terms, two fixed and two random, with one interaction. 

$y_{ijk} = \mu + V_s + \alpha_i + \beta_{(si)j} + T_k + (VT)_{sk} + \varepsilon_{sijk}$, where: 

$y_{ijk}$ = Species Composition
$\mu$ = Grand Mean of Species Composition 
$V_s$ = Factor $V$ (Viereck Classification with $_s$ levels)
$\alpha_i$ = Factor $\alpha$ (Park) with $_i$ levels 
$\beta_{(si)j$ = Factor $\beta$ (Plot) with $_j$ levels, which are nested in the combination of $\alpha$ and $V$
$T_k$ = Factor $T$ (Time) with $_k$ levels 
$(VT)_{sk}$ = Interaction of Factor $V$ and Factor $T$ with $_{sk}$ levels 
$\varepsilon_{sijk}$ = Residual Variation 

1. For each determinant subscript (subscript in parentheses) that matches the subscript in the column, write a 1
2. If any subscripts in a row match the subscript in a column, write a 0 if the column is fixed and a 1 if the column is random. 
3. For the remaining boxes, write the number of levels from that column 

\begin{table}
\centering
\begin{tabular}{|1|1|1|1|1|}
\hline
\textbf{Parameter} & \textbf{$V$} & \textbf{$\alpha$} & \textbf{$\beta$} & \textbf{$T$} \\
\hline
Fixed or Random & F & F & R & R \\
\hline 
Number of Levels & $w$ & $a$ & $b$ & $r$ \\
\hline 
Subscript & $s$ & $i$ & $j$ & $k$ \\
\hline 
Viereck Class $V_s$ & $0$ & $a$ & $b$ & $r$ \\
\hline 
Park $\alpha_i$ & $w$ & $0$ & $b$ & $r$ \\
\hline 
Plot $\beta_{(si)j$ & $1$ & $1$ & $1$ & $r$ \\
\hline 
Time $T_k$ & $w$ & $a$ & $b$ & $1$ \\
\hline 
Interaction $(VT)_{sk}$ & $0$ & $a$ & $b$ & $1$ \\
\hline 
Residual Error $\varepsilon_{sijk}$ & $1$ & $1$ & $1$ & $1$ \\
\hline 
\end{tabular}
\caption{Input matrix for partitioning EMS}
\end{table}

To partition the variance, cover the column of the factor you're working on (go row by row). The product of visible numbers for each column with at least the subscripts from that row is the variance. 

Therefore, for this model, the variance is partitioned like so: 

\begin{table}
\centering
\begin{tabular}{|1|1|}
\hline 
\textbf{Term} & \textbf{Expected Mean Squared Variance} \\
\hline 
Viereck Class $V_s$ & $abr \sum \frac{\alpha i^2}{a-1} + r\sigma^2_{\beta} + ab\sigma^2_{VT} + \sigma^2_{\epsilon}$\\
\hline 
Park $\alpha_i$ & $wbr\sigma^2_{\alpha} + r\sigma^2_{\beta} + \sigma^2_{\epsilon}$ \\
\hline 
Plot $\beta_{(si)j$ & $r\sigma^2_{\beta} + \sigma^2_{\epsilon}$ \\
\hline 
Time $T_k$ & $wab\sigma^2_{T} + ab\sigma^2_{VT} + \sigma^2_{\epsilon}$ \\
\hline 
Interaction $(VT)_{sk}$ & $ab\sigma^2_{VT} + \sigma^2_{\epsilon}$ \\
\hline 
Residual Error $\varepsilon_{sijk}$ & $\sigma^2_{\epsilon}$ \\
\hline 
\end{tabular}
\caption{Partitioned Variance for each variable}
\end{table}

Based on the variance partitioning from the above table, the most accurate F tests for each term would be: 


\begin{table}
\centering
\begin{tabular}{|1|1|}
\hline 
\textbf{Term} & \textbf{Associated F Equations} \\
\hline 
Viereck Class $V_s$ & $F = \frac{abr \sum \frac{\alpha i^2}{a-1} + r\sigma^2_{\beta} + ab\sigma^2_{VT} + \sigma^2_{\epsilon}}{r\sigma^2_{\beta} + \sigma^2_{\epsilon}} = MS_{Viereck}/MS_{Plot}$\\
\hline 
Park $\alpha_i$ & $F = \frac{wbr\sigma^2_{\alpha} + r\sigma^2_{\beta} + \sigma^2_{\epsilon}}{r\sigma^2_{\beta} +\sigma^2_{\epsilon}} = MS_{Park}/MS_{Plot}$ \\
\hline 
Plot $\beta_{(si)j}$ & $\frac{r\sigma^2_{\beta} + \sigma^2_{\epsilon}}{\sigma^2_{\epsilon}} = MS_{Plot}/MS{Residual}$ \\
\hline 
Time $T_k$ & $F = \frac{wab\sigma^2_{T} + ab\sigma^2_{VT} + \sigma^2_{\epsilon}}{\sigma^2_{\epsilon}} = MS_{Time}/MS_{Residual}$ \\
\hline 
Interaction $(VT)_{sk}$ & $F = \frac{ab\sigma^2_{VT} + \sigma^2_{\epsilon}}{\sigma^2_{\epsilon}} = MS_{Interaction}/MS_{Residual}$ \\
\hline 
Residual Error $\varepsilon_{sijk}$ & Not Applicable \\
\hline 
\end{tabular}
\caption{Most accurate F tests for each term in defined model. Note: This assumes the contribution of $VT_{sk}$ is nearly zero and can be zeroed out (Threshold = $R^2 < 0.05$)
\end{table}

Therefore, using the above F equations, we would choose to adjust our F equations when running our adjusted PERMANOVA using the associated function in permanovaTools. 

To test the effects of Viereck Class and Park, we will use run_permanova to adjust our F equations to term/plot instead of the default of term/residual for the most accurate test. 


Varpart() in Vegan partitions raw data variance using Euclidian distances, which is useful in multivariate methods such as RDA and CCA, but for PERMANOVA, which typically uses distances such as Bray-Curtis and Jaccard (not Euclidian), these dissimilarities don't translate. 





Balancing rationale 

Correctly structuring the F tests is only part of ensuring accurate results when using PERMANOVA. Another key space to ensure accuracy is in the data structure itself by balancing strata. To create the null distribution that data is compared against, these data are shuffled (permuted) to erase the effects of interest so that it can be tested against what would happen randomly/by chance. This is where restricting permutations comes into play, allowing you to select how your data are permuted. we are assuming that the permutations are respecting the structure of our experiment so that the null distribution is something that could happen by chance. 

If the experimental structure is imbalanced, one part of the design (ie a plot with more visits) would contribute more to the distribution than the other and the variance partitioning wolud be biased towards it. The pseudo-F test may reflect these differences in structure rather than teh effect being tested. Because the sums of squares in PERMANOVA are based on squared distances between centroids, the centroid of a group with ten visits would be estimated more precisely than a group with only two visits. If each group contributes the same number of visits, then every plot has equal weight in the generation of the null distribution and it makes hte conclusions you can draw from the tests are more accurate. Using a balanced distribution, your denominator variance represents an accurate expectation. Using an imbalanced distribution may lead to underestimated variance among certain factors which would lead to an inflated F statistic. This would in turn lead to overconfidence in the term effects, leading to Type 1 error. It would also lead to the inability to accurately separate the effect of the tested variable from the effect of sampling bias. 

Generally, sample designs are balanced in their design and if all goes to plan, they do not require additional filtering to be properly analyzed. This process comes into play when external, uncontrolled factors disrupt sampling and cause data to not be collected or to have to be thrown out. These factors include plots being inaccessible, travel constraints, unexpected disturbances such as fire, floods, or snowstorms, loss of funding for data collection and hiring, lost data, sampling error leading to rejection of data collected, etc. In these instances, where you set out to collect 5 revisits of each plot but now you have one plot with only 4 revisits, you can make a choice to reject that plot from the design altogether or remove one visit from your other plots so every plot has four visits. 

In principle, sampling designs are constructed to be balanced, so under ideal conditions no additional filtering is needed for analysis. In practice, however, unforseen factors can disrupt data collection and lead to missing or unusable samples. These disruptions include (but are not limited to) inaccessible plots, travel constraints, disturbances such as fire or flooding, loss of project funding, or data loss. In these instances where you unexpectedly find yourself with unbalanced observations, the choice is to either exclude the incomplete unit (ie plot with less revisits than the rest of the plots) from the design or reduce the other units to match the incomplete one. This action would restore balance to the design and allow for accurate and unbiased analysis despite the disruption. This can be done by hand but for large datasets common in species composition surveys it can be tedious and time consuming to manually filter and remove data, so using a function that allows for a combination of auto-selection and manual selection of data can save time while still preserving control over experimental structure. 




